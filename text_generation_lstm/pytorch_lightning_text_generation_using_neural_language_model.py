# -*- coding: utf-8 -*-
"""PyTorch_Lightning_Text_Generation_using_Neural_Language_Model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wAemU_CZSeuzE9vehDi-yxh6ALlDdOc1

# Table of Contents

1. Import Libraries
2. Load Dataset
3. Preprocessing and Exploring Text Data
  
  3.1 Text Cleaning
  
  3.2 Finding Word Count

  3.3 Find and Replace Rare Words with "Unknown" Token

4. Data Preparation

  4.1 Prepare Sequences

  4.2 Create Token-Integer Mappings

  4.3 Split Data into Train and Validation Sets

  4.4 Pad Sequences

  4.5 Convert Text Sequences to Integer Sequences
5. Model Building

  5.1 Define Model Architecture
  
  5.2 Start Model Training
6. Text Generation

# 1. Import Libraries
"""

import pandas as pd
import numpy as np
import pickle
import re
import random
from tqdm import tqdm_notebook

import torch
from torch import nn
import torch.nn.functional as F

import numpy as np
from torch.utils.data import DataLoader, Dataset
!pip install pytorch_lightning -qqq
import pytorch_lightning as pl

!pip install tensorboard
from pytorch_lightning.loggers import TensorBoardLogger

# reproducing same results
SEED = 2019

# torch
torch.manual_seed(SEED)

"""# 2. Load Dataset"""

# mounting the drive
from google.colab import drive
drive.mount('/content/drive')

# open text file and read in data
with open("drive/MyDrive/Colab Notebooks/Text Generation using Neural Language Model/Dailog-dataset.dialogs_dataset", "rb") as f:
  dialogs = pickle.load(f)

"""Note: This is a subset of the Taskmaster dataset available under the Creative Commons Attribution 4.0 License. A full copy of the license can be found at https://creativecommons.org/licenses/by/4.0/. You can access the full dataset from [here](https://github.com/google-research-datasets/Taskmaster/tree/master/TM-1-2019).  """

# number of text sequences
len(dialogs)

# print 10 random dialogs
random.sample(dialogs, 10)

"""# 3. Preprocessing and Exploring Text Data

## 3.1 Text Cleaning
"""

# text cleaning
dialogs_clean = []

for i in dialogs:
  # remove everything except alphabets
  i = re.sub("[^a-zA-Z' ]", "", i)
  # convert text to lowercase
  i = i.lower()
  # add cleaned text to the list
  dialogs_clean.append(i)

random.sample(dialogs_clean, 10)

"""
## 3.2 Finding Word Count"""

# get list of all the words
all_words = " ".join(dialogs_clean).split()

words_dict = {}

# add word-count pair to the dictionary
for word in all_words:
  # check if the word is already in dictionary
  if word in words_dict:
    # increment count of word by 1
    words_dict[word] = words_dict[word] + 1
  else:
    # add the word to dictionary with count 1
    words_dict[word] = 1

# prepare a dataframe
words_df = pd.DataFrame({'word':list(words_dict.keys()), 'count':list(words_dict.values())})

# sort words by their count in increasing order
words_df = words_df.sort_values(by = ['count'])

# reset dataframe index
words_df.reset_index(inplace = True, drop=True)

# vocabulary size
len(words_df)

words_df.head()

words_df.tail()

"""## 3.3 Find and Replace Rare Words with "Unknown" Token"""

# user specified threshold value
rare_thresh = 4

# get percentage of rare words in the vocabulary
rare_words_count = len(words_df[words_df['count'] < rare_thresh]['word'])
total_words = len(words_df)
rare_dist = rare_words_count / total_words

# coverage percentage of rare words in the corpus
rare_cover = words_df[words_df['count'] < rare_thresh]['count'].sum()/words_df['count'].sum()

print(f"Rare words distribution in the vocabulary: {rare_dist*100:.2f}")
print(f"Rare words coverage in the corpus: {rare_cover*100:.2f}")

# extract rare words in a list
rare_words = words_df[words_df['count'] < rare_thresh]['word'].tolist()

"""Let's see the technique that we will use to replace the rare words/tokens in the dataset with a special token known as the unknown token ("\<unk\>")"""

## example
# specify rare words
r_words = ["day", "book"]

# build pattern
pattern = ""
for i in r_words:
  pattern+= "{}|".format(i)

print(pattern)

# removing the last element which is "|"
pattern = pattern[:-1]
print(pattern)

# replace the rare words with the <unk> token
sents = ["it has been a long day", "this book is a must read"]

for d in sents:
  text = re.sub(pattern, " <unk> ", d)
  print(text)

# create a text pattern from the rare words, like "word1 | word2 | word3..."
pattern = ""
for i in rare_words:
  pattern+= " {} |".format(i)

# removing the last element which is "|"
pattern = pattern[:-1]

# empty list
dialogs_clean_v2 = []

# replace the rare words with the <unk> token
for d in tqdm_notebook(dialogs_clean):
  text = re.sub(pattern, " <unk> ", d)
  dialogs_clean_v2.append(text)

dialogs_clean_v2[520:530]

"""# 4. Data Preparation

## 4.1 Prepare Sequences
"""

# capture length of all the sequences
text_word_count = []
for i in dialogs_clean_v2:
  text_word_count.append(len(i.split()))

# plot the sequence lengths
pd.Series(text_word_count).hist(bins = 30,range=(0,30))

# function to create sequences of equal length
def create_seq(text, seq_len = 5):

  sequences = []

  if len(text.split()) > seq_len:
    for i in range(seq_len, len(text.split())):
      # select sequence of tokens
      seq = text.split()[i-seq_len:i+1]
      # append sequence to the list
      sequences.append(" ".join(seq))

    return sequences

  else:

    return [text]

# create sequences of equal length
seqs = [create_seq(i) for i in dialogs_clean_v2]

seqs[:10]

# merge list-of-lists into a single list
seqs = sum(seqs, [])

seqs[:15]

# count of sequences
len(seqs)

# create input and target sequences (x and y)
x = []
y = []

for s in seqs:
  x.append(" ".join(s.split()[:-1]))
  y.append(" ".join(s.split()[1:]))

x[0], y[0]

x[88543], y[88543]

"""## 4.2 Create Token-Integer Mappings"""

# create integer-to-token mapping
int2token = {}
cnt = 1

for w in set(" ".join(dialogs_clean_v2).split()):
  int2token[cnt] = w
  cnt+= 1

# create token-to-integer mapping
token2int = {t: i for i, t in int2token.items()}

token2int["can"], int2token[1127]

"""## 4.3 Split Data into Train and Validation Sets"""

# train-validation split
# input sequences
x_tr = x[:150000]
x_val = x[150000:]

# target sequences
y_tr = y[:150000]
y_val = y[150000:]

"""## 4.4 Pad Sequences"""

# plot sequence length in train set
text_word_count = []

for i in x_tr:
  text_word_count.append(len(i.split()))

pd.Series(text_word_count).hist(bins = 70,range=(0,30))

# based on the plot above
max_text_len = 5

# function to perform padding
def pad_sequence(seq, n):

  # split input sequence into tokens
  seq = seq.split()

  # check if no. of tokens in input sequence is less than 'n'
  if len(seq) < n:
    for i in range(n - len(seq)):
      seq.append("<pad>")

  return " ".join(seq)

# pad text sequences (train set)
x_tr_padded = [pad_sequence(s, max_text_len) for s in x_tr]
y_tr_padded = [pad_sequence(s, max_text_len) for s in y_tr]

# pad text sequences (validation set)
x_val_padded = [pad_sequence(s, max_text_len) for s in x_val]
y_val_padded = [pad_sequence(s, max_text_len) for s in y_val]

x_tr_padded[:20]

y_tr_padded[:20]

exist_count = 0
for i in range(len(y_tr_padded)):
    if "<unk>" in y_tr_padded[i]:
        pass
        #print("exists in ", i, " " ,y_tr_padded[i])
    exist_count += y_tr_padded[i].count("<unk>")
print("exist_count", exist_count)

# update mapping dictionaries
int2token[0] = "<pad>"
token2int["<pad>"] = 0

# set vocabulary size
vocab_size = len(int2token)

"""## 4.5 Convert Text Sequences to Integer Sequences"""

# function to create integer sequences
def get_integer_seq(seq):
  return [token2int[w] for w in seq.split()]

# convert text sequences to integer sequences
x_tr_int = [get_integer_seq(i) for i in x_tr_padded]
y_tr_int = [get_integer_seq(i) for i in y_tr_padded]

x_val_int = [get_integer_seq(i) for i in x_val_padded]
y_val_int = [get_integer_seq(i) for i in y_val_padded]

x_tr_int[:10]

y_tr_int[:10]

# convert lists into numpy arrays
x_tr_int = np.array(x_tr_int)
y_tr_int = np.array(y_tr_int)

x_val_int = np.array(x_val_int)
y_val_int = np.array(y_val_int)

x_tr_int.shape, y_tr_int.shape, x_val_int.shape, y_val_int.shape

"""# 5. Model Building

## 5.1 Define Model Architecture
"""

# define model architecture

## embedding layer:
##    input dim = vocab_size,
##    ouput dim = 200

## LSTM layer:
##    input dim = 200
##    hidden units = 256
##    layers = 2
##    output dim = 256

## Dropout Layer
##    input dim = 256
##    output dim = 256

## fully connected layer
##    input dim = 256
##    ouput dim = vocab_size

class WordLSTM(nn.Module):

  def __init__(self, n_hidden=256, n_layers=2, drop_prob=0.3, lr=0.001):
    super().__init__()
    self.drop_prob = drop_prob
    self.n_layers = n_layers
    self.n_hidden = n_hidden
    self.lr = lr

    self.emb_layer = nn.Embedding(vocab_size, 200)

    ## define the LSTM
    # input data is of shape (batch size, sequence length, no. of features)...
    # ...therefore we need batch_first=True
    self.lstm = nn.LSTM(200, n_hidden, n_layers, batch_first=True)

    ## define a dropout layer
    self.dropout = nn.Dropout(drop_prob)

    ## define the fully-connected layer
    self.fc = nn.Linear(n_hidden, vocab_size)

  def forward(self, x, hidden):
    ''' Forward pass through the network.
        These inputs are x, and the hidden/cell state is `hidden`. '''

    ## pass input through embedding layer
    embedded = self.emb_layer(x)

    ## Get the outputs and the new hidden state from the lstm
    lstm_output, hidden = self.lstm(embedded, hidden)

    ## pass through a dropout layer
    out = self.dropout(lstm_output)

    ## reshape the tensor to the shape (batch-size*sequence length, hidden units)
    out = out.reshape(-1, self.n_hidden)

    ## put "out" through the fully-connected layer
    out = self.fc(out)

    # return the final output and the hidden state
    return out, hidden


  def init_hidden(self, batch_size):
    ''' Initializes hidden state '''
    # Create two new tensors with sizes n_layers x batch_size x n_hidden,
    # initialized to zero, for hidden state and cell state of LSTM
    weight = next(self.parameters()).data

    if (torch.cuda.is_available()):
      hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),
                weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())
    else:
      hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),
                weight.new(self.n_layers, batch_size, self.n_hidden).zero_())

    return hidden

# define and print the net
net = WordLSTM()
print(net)

# function to generate batches
def get_batches(arr_x, arr_y, batch_size):
  # iterate through the arrays
  prv = 0

  for n in range(batch_size, arr_x.shape[0], batch_size):
    # batch of input sequences
    x = arr_x[prv:n,:]

    # batch of target sequences
    y = arr_y[prv:n,:]

    prv = n

    yield x, y

"""## 5.2 Start Model Training"""

def train(net, epochs=10, batch_size=32, lr=0.001, print_every=32):

  # set initial loss to infinite
  best_valid_loss = float('inf')

  # optimizer
  opt = torch.optim.Adam(net.parameters(), lr=lr)

  # loss function
  criterion = nn.CrossEntropyLoss()

  if(torch.cuda.is_available()):
    # push model to GPU
    net.cuda()

  counter = 0

  net.train()

  for e in range(epochs):


    # iterate over batches
    for x, y in get_batches(x_tr_int, y_tr_int, batch_size):
      counter+= 1

      # convert arrays to tensors
      inputs, targets = torch.from_numpy(x), torch.from_numpy(y)

      if(torch.cuda.is_available()):
        # push tensors to GPU
        inputs, targets = inputs.cuda(), targets.cuda()

      # initialize hidden state
      h = net.init_hidden(batch_size)

      # set accumulated gradients to zero
      net.zero_grad()

      # get the output from the model
      output, h = net(inputs, h)

      # calculate the loss and perform backprop
      loss = criterion(output, targets.view(-1))
      loss.backward()

      opt.step()

      if counter % print_every == 0:
        # Get validation loss

        val_losses = []

        net.eval()
        for x, y in get_batches(x_val_int, y_val_int, batch_size):

          x, y = torch.from_numpy(x), torch.from_numpy(y)

          val_h = net.init_hidden(batch_size)

          inputs, targets = x, y
          if(torch.cuda.is_available()):
            inputs, targets = inputs.cuda(), targets.cuda()

          output, val_h = net(inputs, val_h)

          val_loss = criterion(output, targets.view(-1))
          val_losses.append(val_loss.item())

        #save the best model
        if np.mean(val_losses) < best_valid_loss:
          best_valid_loss = np.mean(val_losses)
          torch.save(net.state_dict(), 'saved_weights.pt')

        net.train()


        print("Epoch: {}/{}...".format(e+1, epochs),
              "Step: {}...".format(counter),
              "Loss: {:.4f}...".format(loss.item()),
              "ppl: {:.4f} ".format(np.exp(np.mean(val_losses))),
              "Val Loss: {:.4f}".format(np.mean(val_losses)))

# specify batch size
batch_size = 64

# train the model
train(net, batch_size = batch_size, epochs=10)

"""# 6. Text Generation"""

#load weights of best model
path = 'saved_weights.pt'
net.load_state_dict(torch.load(path))

# function to generate one token
def predict(net, tkn, h=None):

  # tensor inputs
  x = np.array([[token2int[tkn]]])
  inputs = torch.from_numpy(x)

  if(torch.cuda.is_available()):
      inputs = inputs.cuda()

  # get the output of the model
  out, h = net(inputs, h)

  # get the token probabilities
  p = F.softmax(out, dim=1).data

  if(torch.cuda.is_available()):
      p = p.cpu()

  p = p.numpy()
  sampled_token_index = np.argmax(p, axis = 1)[0]

  # return the encoded value of the predicted char and the hidden state
  return int2token[sampled_token_index], h

# function to fetch generated sequence
def sample(net, size = 2, seed_text='it is'):

    if(torch.cuda.is_available()):
        net.cuda()

    net.eval()

    # batch size is 1
    h = net.init_hidden(1)

    toks = seed_text.split()

    # predict next token
    for t in toks:
      token, h = predict(net, t, h)

    toks.append(token)

    # predict subsequent tokens
    for i in range(size-1):
        token, h = predict(net, toks[-1], h)
        toks.append(token)

    return ' '.join(toks)

# seed texts
seeds = ["i want to",
         "how about a cup",
         "i don't want",
         "can you send",
         "my car"]

# number of tokens to generate
num_toks = 6

# text generation
for s in seeds:
  # get generated text from the model
  text_gen = sample(net, num_toks, seed_text=s)
  # print the result
  print("seed text:", s, ">> output:",text_gen)
  print("\n")

"""# Converting above Pytorch model to Pytorch Lightning"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import pytorch_lightning as pl

class WordLSTM_pl(pl.LightningModule):
    def __init__(self, vocab_size, n_hidden=256, n_layers=2, drop_prob=0.3, lr=0.001):
        super().__init__()
        self.drop_prob = drop_prob
        self.n_layers = n_layers
        self.n_hidden = n_hidden
        self.lr = lr

        self.emb_layer = nn.Embedding(vocab_size, 200)
        self.lstm = nn.LSTM(200, n_hidden, n_layers, batch_first=True)
        self.dropout = nn.Dropout(drop_prob)
        self.fc = nn.Linear(n_hidden, vocab_size)

    def forward(self, x, hidden):
        embedded = self.emb_layer(x)
        lstm_output, hidden = self.lstm(embedded, hidden)
        out = self.dropout(lstm_output)
        out = out.reshape(-1, self.n_hidden)
        out = self.fc(out)
        return out, hidden

    def init_hidden(self, batch_size):
        weight = next(self.parameters()).data
        if torch.cuda.is_available():
            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),
                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())
        else:
            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),
                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())
        return hidden

    def training_step(self, batch, batch_idx):
        x, y = batch
        batch_size = x.size(0)
        h = self.init_hidden(batch_size)
        output, _ = self(x, h)
        loss = F.cross_entropy(output, y.view(-1))
        self.log('train_loss', loss)
        return loss

    def validation_step(self, batch, batch_idx):
        x, y = batch
        batch_size = x.size(0)
        h = self.init_hidden(batch_size)
        output, _ = self(x, h)
        loss = F.cross_entropy(output, y.view(-1))
        self.log('val_loss', loss)
        return loss

    def configure_optimizers(self):
        return torch.optim.Adam(self.parameters(), lr=self.lr)

# convert text sequences to integer sequences
x_tr_int_pl = [get_integer_seq(i) for i in x_tr_padded]
y_tr_int_pl = [get_integer_seq(i) for i in y_tr_padded]

x_val_int_pl = [get_integer_seq(i) for i in x_val_padded]
y_val_int_pl = [get_integer_seq(i) for i in y_val_padded]

x_tr_int_pl[:10]

class CustomDataset(torch.utils.data.Dataset):
    def __init__(self, x, y):
        self.x = torch.from_numpy(np.asarray(x)).long()
        self.y = torch.from_numpy(np.asarray(y)).long() #Try torch.from_numpy(np.asarray(x)).


    def __len__(self):
        return len(self.x)

    def __getitem__(self, idx):
        return self.x[idx], self.y[idx]

# Define batch size and other hyperparameters
batch_size = 64
vocab_size = len(int2token)

# Create DataLoader objects for training and validation datasets
train_loader = torch.utils.data.DataLoader(CustomDataset(x_tr_int_pl, y_tr_int_pl), batch_size=batch_size, shuffle=True)
val_loader = torch.utils.data.DataLoader(CustomDataset(x_val_int_pl, y_val_int_pl), batch_size=batch_size)

# Create an instance of your WordLSTM model
model = WordLSTM_pl(vocab_size)

# Define a callback to save the best model during training
checkpoint_callback = pl.callbacks.ModelCheckpoint(
    dirpath='checkpoints',
    filename='best_model',
    monitor='val_loss',
    mode='min',
    save_top_k=1
)

# Define a TensorBoardLogger to log metrics
logger = TensorBoardLogger("logs", name="word_lstm_logs")

# Create a PyTorch Lightning Trainer object with the checkpoint callback
trainer = pl.Trainer(
    max_epochs=10,
    accelerator="auto",
    callbacks=[checkpoint_callback],
    logger=logger
)

# Train the model
trainer.fit(model, train_loader, val_loader)

trainer.save_checkpoint("checkpoints/model_checkpoint.ckpt")

# Load the best model from the checkpoint
#best_model_path = checkpoint_callback.best_model_path
#best_model = WordLSTM_pl.load_from_checkpoint(checkpoint_path=best_model_path)
#new_model = WordLSTM_pl.load_from_checkpoint(checkpoint_path="checkpoints/model_checkpoint.ckpt")


# Function to generate one token
def predict(net, tkn, h=None):
    x = np.array([[token2int[tkn]]])
    inputs = torch.from_numpy(x).long()
    if torch.cuda.is_available():
        inputs = inputs.cuda()
    out, h = net(inputs, h)
    p = F.softmax(out, dim=1).data
    if torch.cuda.is_available():
        p = p.cpu()
    p = p.numpy()
    sampled_token_index = np.argmax(p, axis=1)[0]
    return int2token[sampled_token_index], h

# Function to fetch generated sequence
def sample(net, size=2, seed_text='it is'):
    if torch.cuda.is_available():
        net.cuda()
    net.eval()
    h = net.init_hidden(1)
    toks = seed_text.split()
    for t in toks:
        token, h = predict(net, t, h)
    toks.append(token)
    for i in range(size - 1):
        token, h = predict(net, toks[-1], h)
        toks.append(token)
    return ' '.join(toks)

# seed texts
seeds = ["i want to",
         "how about a cup",
         "i don't want",
         "can you send",
         "my car"]

# number of tokens to generate
num_toks = 6

# text generation
for s in seeds:
  # get generated text from the model
  text_gen1 = sample(net, num_toks, seed_text=s)
  text_gen2 = sample(model, num_toks, seed_text=s)
  # print the result
  print("pytorch: seed text:", s, ">> output:",text_gen1)
  print("pytorch lightning: seed text:", s, ">> output:",text_gen2)
  print("\n")

